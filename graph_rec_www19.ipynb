{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea296c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df906daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a0f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict\n",
    "#from UV_Encoders import UV_Encoder\n",
    "#from UV_Aggregators import UV_Aggregator\n",
    "#from Social_Encoders import Social_Encoder\n",
    "#from Social_Aggregators import Social_Aggregator\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt\n",
    "import datetime\n",
    "import argparse\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47225128",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "## toy dataset \n",
    "history_u_lists, history_ur_lists:  user's purchased history (item set in training set), and his/her rating score (dict)\n",
    "history_v_lists, history_vr_lists:  user set (in training set) who have interacted with the item, and rating score (dict)\n",
    "\n",
    "train_u, train_v, train_r: training_set (user, item, rating)\n",
    "test_u, test_v, test_r: testing set (user, item, rating)\n",
    "\n",
    "# please add the validation set\n",
    "\n",
    "social_adj_lists: user's connected neighborhoods\n",
    "ratings_list: rating value from 0.5 to 4.0 (8 opinion embeddings)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91dda624",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "use_cuda = False\n",
    "if torch.cuda.is_available():\n",
    "    use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "embed_dim = 128\n",
    "dir_data = './data/toy_dataset'\n",
    "\n",
    "path_data = dir_data + \".pickle\"\n",
    "data_file = open(path_data, 'rb')\n",
    "history_u_lists, history_ur_lists, history_v_lists, history_vr_lists, train_u, train_v, train_r, test_u, test_v, test_r, social_adj_lists, ratings_list = pickle.load(\n",
    "    data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13177788",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.TensorDataset(torch.LongTensor(train_u), torch.LongTensor(train_v),\n",
    "                                              torch.FloatTensor(train_r))\n",
    "testset = torch.utils.data.TensorDataset(torch.LongTensor(test_u), torch.LongTensor(test_v),\n",
    "                                         torch.FloatTensor(test_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "018928c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "test_batch_size = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "# 705명 유저\n",
    "num_users = history_u_lists.__len__()\n",
    "\n",
    "# 1941개 아이템 수 \n",
    "num_items = history_v_lists.__len__()\n",
    "\n",
    "# 8등급, 2.0: 0, 1.0: 1, 3.0: 2, 4.0: 3, 2.5: 4, 3.5: 5, 1.5: 6, 0.5: 7\n",
    "num_ratings = ratings_list.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "202d3427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2.0: 0, 1.0: 1, 3.0: 2, 4.0: 3, 2.5: 4, 3.5: 5, 1.5: 6, 0.5: 7}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79cd2341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_dim = 128차원 \n",
    "\n",
    "u2e = nn.Embedding(num_users, embed_dim).to(device)\n",
    "v2e = nn.Embedding(num_items, embed_dim).to(device)\n",
    "r2e = nn.Embedding(num_ratings, embed_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfdf2db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(705, 128), Embedding(1941, 128), Embedding(8, 128))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u2e, v2e, r2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2ef254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embedding_dims):\n",
    "        super(Attention, self).__init__()\n",
    "        self.embed_dim = embedding_dims\n",
    "        self.bilinear = nn.Bilinear(self.embed_dim, self.embed_dim, 1)\n",
    "        self.att1 = nn.Linear(self.embed_dim * 2, self.embed_dim)\n",
    "        self.att2 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.att3 = nn.Linear(self.embed_dim, 1)\n",
    "        self.softmax = nn.Softmax(0)\n",
    "\n",
    "    def forward(self, node1, u_rep, num_neighs):\n",
    "        uv_reps = u_rep.repeat(num_neighs, 1)\n",
    "        x = torch.cat((node1, uv_reps), 1)\n",
    "        x = F.relu(self.att1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.att2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.att3(x)\n",
    "        att = F.softmax(x, dim=0)\n",
    "        return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e243c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "#from Attention import Attention\n",
    "\n",
    "\n",
    "class UV_Aggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    item and user aggregator: for aggregating embeddings of neighbors (item/user aggreagator).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, v2e, r2e, u2e, embed_dim, cuda=\"cpu\", uv=True):\n",
    "        super(UV_Aggregator, self).__init__()\n",
    "        self.uv = uv\n",
    "        self.v2e = v2e\n",
    "        self.r2e = r2e\n",
    "        self.u2e = u2e\n",
    "        self.device = cuda\n",
    "        self.embed_dim = embed_dim\n",
    "        self.w_r1 = nn.Linear(self.embed_dim * 2, self.embed_dim)\n",
    "        self.w_r2 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.att = Attention(self.embed_dim)\n",
    "\n",
    "    def forward(self, nodes, history_uv, history_r):\n",
    "\n",
    "        embed_matrix = torch.empty(len(history_uv), self.embed_dim, dtype=torch.float).to(self.device)\n",
    "\n",
    "        for i in range(len(history_uv)): \n",
    "            history = history_uv[i]\n",
    "            num_histroy_item = len(history)\n",
    "            tmp_label = history_r[i]\n",
    "\n",
    "            if self.uv == True:\n",
    "                # user component\n",
    "                e_uv = self.v2e.weight[history]\n",
    "                uv_rep = self.u2e.weight[nodes[i]]\n",
    "            else:\n",
    "                # item component\n",
    "                e_uv = self.u2e.weight[history]\n",
    "                uv_rep = self.v2e.weight[nodes[i]]\n",
    "\n",
    "            e_r = self.r2e.weight[tmp_label]\n",
    "            x = torch.cat((e_uv, e_r), 1)\n",
    "            x = F.relu(self.w_r1(x))\n",
    "            o_history = F.relu(self.w_r2(x))\n",
    "\n",
    "            att_w = self.att(o_history, uv_rep, num_histroy_item)\n",
    "            att_history = torch.mm(o_history.t(), att_w)\n",
    "            att_history = att_history.t()\n",
    "\n",
    "            embed_matrix[i] = att_history\n",
    "        to_feats = embed_matrix\n",
    "        return to_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7139c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_u_history = UV_Aggregator(v2e, r2e, u2e, embed_dim, cuda=device, uv=True) # user representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7aa02d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UV_Aggregator(\n",
       "  (v2e): Embedding(1941, 128)\n",
       "  (r2e): Embedding(8, 128)\n",
       "  (u2e): Embedding(705, 128)\n",
       "  (w_r1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (w_r2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (att): Attention(\n",
       "    (bilinear): Bilinear(in1_features=128, in2_features=128, out_features=1, bias=True)\n",
       "    (att1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (att2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (att3): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (softmax): Softmax(dim=0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_u_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eea18c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class UV_Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features, embed_dim, history_uv_lists, history_r_lists, aggregator, cuda=\"cpu\", uv=True):\n",
    "        super(UV_Encoder, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.uv = uv\n",
    "        self.history_uv_lists = history_uv_lists\n",
    "        self.history_r_lists = history_r_lists\n",
    "        self.aggregator = aggregator\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = cuda\n",
    "        self.linear1 = nn.Linear(2 * self.embed_dim, self.embed_dim)  #\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        tmp_history_uv = []\n",
    "        tmp_history_r = []\n",
    "        for node in nodes:\n",
    "            tmp_history_uv.append(self.history_uv_lists[int(node)])\n",
    "            tmp_history_r.append(self.history_r_lists[int(node)])\n",
    "\n",
    "        neigh_feats = self.aggregator.forward(nodes, tmp_history_uv, tmp_history_r)  # user-item network\n",
    "\n",
    "        self_feats = self.features.weight[nodes]\n",
    "        # self-connection could be considered.\n",
    "        combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
    "        combined = F.relu(self.linear1(combined))\n",
    "\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c3ab831",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_u_history = UV_Encoder(u2e, embed_dim, history_u_lists, history_ur_lists, agg_u_history, cuda=device, uv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4f64e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UV_Encoder(\n",
       "  (features): Embedding(705, 128)\n",
       "  (aggregator): UV_Aggregator(\n",
       "    (v2e): Embedding(1941, 128)\n",
       "    (r2e): Embedding(8, 128)\n",
       "    (u2e): Embedding(705, 128)\n",
       "    (w_r1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (w_r2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (att): Attention(\n",
       "      (bilinear): Bilinear(in1_features=128, in2_features=128, out_features=1, bias=True)\n",
       "      (att1): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (att2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (att3): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (softmax): Softmax(dim=0)\n",
       "    )\n",
       "  )\n",
       "  (linear1): Linear(in_features=256, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_u_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c996b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "#from Attention import Attention\n",
    "\n",
    "\n",
    "class Social_Aggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    Social Aggregator: for aggregating embeddings of social neighbors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, u2e, embed_dim, cuda=\"cpu\"):\n",
    "        super(Social_Aggregator, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.device = cuda\n",
    "        self.u2e = u2e\n",
    "        self.embed_dim = embed_dim\n",
    "        self.att = Attention(self.embed_dim)\n",
    "\n",
    "    def forward(self, nodes, to_neighs):\n",
    "        embed_matrix = torch.empty(len(nodes), self.embed_dim, dtype=torch.float).to(self.device)\n",
    "        for i in range(len(nodes)):\n",
    "            tmp_adj = to_neighs[i]\n",
    "            num_neighs = len(tmp_adj)\n",
    "            # \n",
    "            e_u = self.u2e.weight[list(tmp_adj)] # fast: user embedding \n",
    "            #slow: item-space user latent factor (item aggregation)\n",
    "            #feature_neigbhors = self.features(torch.LongTensor(list(tmp_adj)).to(self.device))\n",
    "            #e_u = torch.t(feature_neigbhors)\n",
    "\n",
    "            u_rep = self.u2e.weight[nodes[i]]\n",
    "\n",
    "            att_w = self.att(e_u, u_rep, num_neighs)\n",
    "            att_history = torch.mm(e_u.t(), att_w).t()\n",
    "            embed_matrix[i] = att_history\n",
    "        to_feats = embed_matrix\n",
    "\n",
    "        return to_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ad6b3cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(nodes)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda nodes: enc_u_history(nodes).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0975236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_u_social = Social_Aggregator(lambda nodes: enc_u_history(nodes).t(), u2e, embed_dim, cuda=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b3ea45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Social_Aggregator(\n",
       "  (u2e): Embedding(705, 128)\n",
       "  (att): Attention(\n",
       "    (bilinear): Bilinear(in1_features=128, in2_features=128, out_features=1, bias=True)\n",
       "    (att1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (att2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (att3): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (softmax): Softmax(dim=0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_u_social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e9991d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Social_Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features, embed_dim, social_adj_lists, aggregator, base_model=None, cuda=\"cpu\"):\n",
    "        super(Social_Encoder, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.social_adj_lists = social_adj_lists\n",
    "        self.aggregator = aggregator\n",
    "        if base_model != None:\n",
    "            self.base_model = base_model\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = cuda\n",
    "        self.linear1 = nn.Linear(2 * self.embed_dim, self.embed_dim)  #\n",
    "\n",
    "    def forward(self, nodes):\n",
    "\n",
    "        to_neighs = []\n",
    "        for node in nodes:\n",
    "            to_neighs.append(self.social_adj_lists[int(node)])\n",
    "        neigh_feats = self.aggregator.forward(nodes, to_neighs)  # user-user network\n",
    "        #print(type(self_feats))\n",
    "        self_feats = self.features(torch.LongTensor(nodes.cpu().numpy())).to(self.device)\n",
    "        #print(type(self_feats))\n",
    "        self_feats = self_feats.t()\n",
    "        \n",
    "        # self-connection could be considered.\n",
    "        combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
    "        combined = F.relu(self.linear1(combined))\n",
    "\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85133a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_u = Social_Encoder(lambda nodes: enc_u_history(nodes).t(), embed_dim, social_adj_lists, agg_u_social, base_model=enc_u_history, cuda=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69ffb10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Social_Encoder(\n",
       "  (aggregator): Social_Aggregator(\n",
       "    (u2e): Embedding(705, 128)\n",
       "    (att): Attention(\n",
       "      (bilinear): Bilinear(in1_features=128, in2_features=128, out_features=1, bias=True)\n",
       "      (att1): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (att2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (att3): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (softmax): Softmax(dim=0)\n",
       "    )\n",
       "  )\n",
       "  (base_model): UV_Encoder(\n",
       "    (features): Embedding(705, 128)\n",
       "    (aggregator): UV_Aggregator(\n",
       "      (v2e): Embedding(1941, 128)\n",
       "      (r2e): Embedding(8, 128)\n",
       "      (u2e): Embedding(705, 128)\n",
       "      (w_r1): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (w_r2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (att): Attention(\n",
       "        (bilinear): Bilinear(in1_features=128, in2_features=128, out_features=1, bias=True)\n",
       "        (att1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (att2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (att3): Linear(in_features=128, out_features=1, bias=True)\n",
       "        (softmax): Softmax(dim=0)\n",
       "      )\n",
       "    )\n",
       "    (linear1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=256, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a40329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # item feature: user * rating\n",
    "agg_v_history = UV_Aggregator(v2e, r2e, u2e, embed_dim, cuda=device, uv=False)\n",
    "enc_v_history = UV_Encoder(v2e, embed_dim, history_v_lists, history_vr_lists, agg_v_history, cuda=device, uv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6236e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRec(nn.Module):\n",
    "\n",
    "    def __init__(self, enc_u, enc_v_history, r2e):\n",
    "        super(GraphRec, self).__init__()\n",
    "        self.enc_u = enc_u\n",
    "        self.enc_v_history = enc_v_history\n",
    "        self.embed_dim = enc_u.embed_dim\n",
    "\n",
    "        self.w_ur1 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.w_ur2 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.w_vr1 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.w_vr2 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.w_uv1 = nn.Linear(self.embed_dim * 2, self.embed_dim)\n",
    "        self.w_uv2 = nn.Linear(self.embed_dim, 16)\n",
    "        self.w_uv3 = nn.Linear(16, 1)\n",
    "        self.r2e = r2e\n",
    "        self.bn1 = nn.BatchNorm1d(self.embed_dim, momentum=0.5)\n",
    "        self.bn2 = nn.BatchNorm1d(self.embed_dim, momentum=0.5)\n",
    "        self.bn3 = nn.BatchNorm1d(self.embed_dim, momentum=0.5)\n",
    "        self.bn4 = nn.BatchNorm1d(16, momentum=0.5)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, nodes_u, nodes_v):\n",
    "        embeds_u = self.enc_u(nodes_u)\n",
    "        embeds_v = self.enc_v_history(nodes_v)\n",
    "\n",
    "        x_u = F.relu(self.bn1(self.w_ur1(embeds_u)))\n",
    "        x_u = F.dropout(x_u, training=self.training)\n",
    "        x_u = self.w_ur2(x_u)\n",
    "        x_v = F.relu(self.bn2(self.w_vr1(embeds_v)))\n",
    "        x_v = F.dropout(x_v, training=self.training)\n",
    "        x_v = self.w_vr2(x_v)\n",
    "\n",
    "        x_uv = torch.cat((x_u, x_v), 1)\n",
    "        x = F.relu(self.bn3(self.w_uv1(x_uv)))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.bn4(self.w_uv2(x)))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        scores = self.w_uv3(x)\n",
    "        return scores.squeeze()\n",
    "\n",
    "    def loss(self, nodes_u, nodes_v, labels_list):\n",
    "        scores = self.forward(nodes_u, nodes_v)\n",
    "        return self.criterion(scores, labels_list)\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, best_rmse, best_mae):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        batch_nodes_u, batch_nodes_v, labels_list = data\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(batch_nodes_u.to(device), batch_nodes_v.to(device), labels_list.to(device))\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print('[%d, %5d] loss: %.3f, The best rmse/mae: %.6f / %.6f' % (\n",
    "                epoch, i, running_loss / 100, best_rmse, best_mae))\n",
    "            running_loss = 0.0\n",
    "    return 0\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    tmp_pred = []\n",
    "    target = []\n",
    "    with torch.no_grad():\n",
    "        for test_u, test_v, tmp_target in test_loader:\n",
    "            test_u, test_v, tmp_target = test_u.to(device), test_v.to(device), tmp_target.to(device)\n",
    "            val_output = model.forward(test_u, test_v)\n",
    "            tmp_pred.append(list(val_output.data.cpu().numpy()))\n",
    "            target.append(list(tmp_target.data.cpu().numpy()))\n",
    "    tmp_pred = np.array(sum(tmp_pred, []))\n",
    "    target = np.array(sum(target, []))\n",
    "    expected_rmse = sqrt(mean_squared_error(tmp_pred, target))\n",
    "    mae = mean_absolute_error(tmp_pred, target)\n",
    "    return expected_rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d6476956",
   "metadata": {},
   "outputs": [],
   "source": [
    " # model\n",
    "    \n",
    "lr = 0.01\n",
    "graphrec = GraphRec(enc_u, enc_v_history, r2e).to(device)\n",
    "optimizer = torch.optim.RMSprop(graphrec.parameters(), lr=lr, alpha=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4788fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rmse = 9999.0\n",
    "best_mae = 9999.0\n",
    "endure_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "969bfcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     0] loss: 0.101, The best rmse/mae: 9999.000000 / 9999.000000\n",
      "[1,   100] loss: 2.059, The best rmse/mae: 9999.000000 / 9999.000000\n",
      "rmse: 2.7770, mae:2.6131 \n",
      "[2,     0] loss: 0.016, The best rmse/mae: 2.777029 / 2.613142\n",
      "[2,   100] loss: 1.169, The best rmse/mae: 2.777029 / 2.613142\n",
      "rmse: 1.1089, mae:0.9705 \n",
      "[3,     0] loss: 0.011, The best rmse/mae: 1.108937 / 0.970456\n",
      "[3,   100] loss: 0.912, The best rmse/mae: 1.108937 / 0.970456\n",
      "rmse: 1.0410, mae:0.9001 \n",
      "[4,     0] loss: 0.009, The best rmse/mae: 1.041034 / 0.900070\n",
      "[4,   100] loss: 0.771, The best rmse/mae: 1.041034 / 0.900070\n",
      "rmse: 0.8653, mae:0.6922 \n",
      "[5,     0] loss: 0.008, The best rmse/mae: 0.865284 / 0.692204\n",
      "[5,   100] loss: 0.731, The best rmse/mae: 0.865284 / 0.692204\n",
      "rmse: 1.0956, mae:0.8152 \n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    train(graphrec, device, train_loader, optimizer, epoch, best_rmse, best_mae)\n",
    "    expected_rmse, mae = test(graphrec, device, test_loader)\n",
    "    # please add the validation set to tune the hyper-parameters based on your datasets.\n",
    "\n",
    "    # early stopping (no validation set in toy dataset)\n",
    "    if best_rmse > expected_rmse:\n",
    "        best_rmse = expected_rmse\n",
    "        best_mae = mae\n",
    "        endure_count = 0\n",
    "    else:\n",
    "        endure_count += 1\n",
    "    print(\"rmse: %.4f, mae:%.4f \" % (expected_rmse, mae))\n",
    "\n",
    "    if endure_count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6771e596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
