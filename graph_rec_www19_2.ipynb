{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ea296c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 26 14:15:11 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:17:00.0 Off |                  N/A |\n",
      "|  0%   35C    P8    16W / 350W |   2063MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:65:00.0 Off |                  N/A |\n",
      "| 30%   47C    P0    99W / 350W |      0MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    335180      C   ...raph-embedding/bin/python     2061MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "df906daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/user/3min/Graph_rec_wwww19'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "41a0f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict\n",
    "#from UV_Encoders import UV_Encoder\n",
    "#from UV_Aggregators import UV_Aggregator\n",
    "#from Social_Encoders import Social_Encoder\n",
    "#from Social_Aggregators import Social_Aggregator\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt\n",
    "import datetime\n",
    "import argparse\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47225128",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "## toy dataset \n",
    "history_u_lists, history_ur_lists:  user's purchased history (item set in training set), and his/her rating score (dict)\n",
    "history_v_lists, history_vr_lists:  user set (in training set) who have interacted with the item, and rating score (dict)\n",
    "\n",
    "train_u, train_v, train_r: training_set (user, item, rating)\n",
    "test_u, test_v, test_r: testing set (user, item, rating)\n",
    "\n",
    "# please add the validation set\n",
    "\n",
    "social_adj_lists: user's connected neighborhoods\n",
    "ratings_list: rating value from 0.5 to 4.0 (8 opinion embeddings)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "91dda624",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "use_cuda = False\n",
    "if torch.cuda.is_available():\n",
    "    use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "embed_dim = 128\n",
    "dir_data = './data/toy_dataset'\n",
    "\n",
    "path_data = dir_data + \".pickle\"\n",
    "data_file = open(path_data, 'rb')\n",
    "history_u_lists, history_ur_lists, history_v_lists, history_vr_lists, train_u, train_v, train_r, test_u, test_v, test_r, social_adj_lists, ratings_list = pickle.load(\n",
    "    data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "13177788",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.TensorDataset(torch.LongTensor(train_u), torch.LongTensor(train_v),\n",
    "                                              torch.FloatTensor(train_r))\n",
    "testset = torch.utils.data.TensorDataset(torch.LongTensor(test_u), torch.LongTensor(test_v),\n",
    "                                         torch.FloatTensor(test_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "018928c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "test_batch_size = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "# 705명 유저\n",
    "num_users = history_u_lists.__len__()\n",
    "\n",
    "# 1941개 아이템 수 \n",
    "num_items = history_v_lists.__len__()\n",
    "\n",
    "# 8등급, 2.0: 0, 1.0: 1, 3.0: 2, 4.0: 3, 2.5: 4, 3.5: 5, 1.5: 6, 0.5: 7\n",
    "num_ratings = ratings_list.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "202d3427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2.0: 0, 1.0: 1, 3.0: 2, 4.0: 3, 2.5: 4, 3.5: 5, 1.5: 6, 0.5: 7}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "79cd2341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_dim = 128차원 \n",
    "\n",
    "u2e = nn.Embedding(num_users, embed_dim).to(device)\n",
    "v2e = nn.Embedding(num_items, embed_dim).to(device)\n",
    "r2e = nn.Embedding(num_ratings, embed_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dfdf2db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(705, 128), Embedding(1941, 128), Embedding(8, 128))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u2e, v2e, r2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e2ef254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embedding_dims):\n",
    "        super(Attention, self).__init__()\n",
    "        self.embed_dim = embedding_dims\n",
    "        self.bilinear = nn.Bilinear(self.embed_dim, self.embed_dim, 1)\n",
    "        self.att1 = nn.Linear(self.embed_dim * 2, self.embed_dim)\n",
    "        self.att2 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.att3 = nn.Linear(self.embed_dim, 1)\n",
    "        self.softmax = nn.Softmax(0)\n",
    "\n",
    "    def forward(self, node1, u_rep, num_neighs):\n",
    "        uv_reps = u_rep.repeat(num_neighs, 1)\n",
    "        x = torch.cat((node1, uv_reps), 1)\n",
    "        x = F.relu(self.att1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.att2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.att3(x)\n",
    "        att = F.softmax(x, dim=0)\n",
    "        return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "43e243c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "#from Attention import Attention\n",
    "\n",
    "\n",
    "class UV_Aggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    item and user aggregator: for aggregating embeddings of neighbors (item/user aggreagator).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, v2e, r2e, u2e, embed_dim, cuda=\"cpu\", uv=True):\n",
    "        super(UV_Aggregator, self).__init__()\n",
    "        self.uv = uv\n",
    "        self.v2e = v2e\n",
    "        self.r2e = r2e\n",
    "        self.u2e = u2e\n",
    "        self.device = cuda\n",
    "        self.embed_dim = embed_dim\n",
    "        self.w_r1 = nn.Linear(self.embed_dim * 2, self.embed_dim)\n",
    "        self.w_r2 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.att = Attention(self.embed_dim)\n",
    "\n",
    "    def forward(self, nodes, history_uv, history_r):\n",
    "\n",
    "        embed_matrix = torch.empty(len(history_uv), self.embed_dim, dtype=torch.float).to(self.device)\n",
    "\n",
    "        for i in range(len(history_uv)): \n",
    "            history = history_uv[i]\n",
    "            num_histroy_item = len(history)\n",
    "            tmp_label = history_r[i]\n",
    "\n",
    "            if self.uv == True:\n",
    "                # user component\n",
    "                e_uv = self.v2e.weight[history]\n",
    "                uv_rep = self.u2e.weight[nodes[i]]\n",
    "            else:\n",
    "                # item component\n",
    "                e_uv = self.u2e.weight[history]\n",
    "                uv_rep = self.v2e.weight[nodes[i]]\n",
    "\n",
    "            e_r = self.r2e.weight[tmp_label]\n",
    "            x = torch.cat((e_uv, e_r), 1)\n",
    "            x = F.relu(self.w_r1(x))\n",
    "            o_history = F.relu(self.w_r2(x))\n",
    "\n",
    "            att_w = self.att(o_history, uv_rep, num_histroy_item)\n",
    "            att_history = torch.mm(o_history.t(), att_w)\n",
    "            att_history = att_history.t()\n",
    "\n",
    "            embed_matrix[i] = att_history\n",
    "        to_feats = embed_matrix\n",
    "        return to_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f7139c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_u_history = UV_Aggregator(v2e, r2e, u2e, embed_dim, cuda=device, uv=True) # user representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e7aa02d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UV_Aggregator(\n",
       "  (v2e): Embedding(1941, 128)\n",
       "  (r2e): Embedding(8, 128)\n",
       "  (u2e): Embedding(705, 128)\n",
       "  (w_r1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (w_r2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (att): Attention(\n",
       "    (bilinear): Bilinear(in1_features=128, in2_features=128, out_features=1, bias=True)\n",
       "    (att1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (att2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (att3): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (softmax): Softmax(dim=0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_u_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eea18c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class UV_Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features, embed_dim, history_uv_lists, history_r_lists, aggregator, cuda=\"cpu\", uv=True):\n",
    "        super(UV_Encoder, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.uv = uv\n",
    "        self.history_uv_lists = history_uv_lists\n",
    "        self.history_r_lists = history_r_lists\n",
    "        self.aggregator = aggregator\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = cuda\n",
    "        self.linear1 = nn.Linear(2 * self.embed_dim, self.embed_dim)  #\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        tmp_history_uv = []\n",
    "        tmp_history_r = []\n",
    "        for node in nodes:\n",
    "            tmp_history_uv.append(self.history_uv_lists[int(node)])\n",
    "            tmp_history_r.append(self.history_r_lists[int(node)])\n",
    "\n",
    "        neigh_feats = self.aggregator.forward(nodes, tmp_history_uv, tmp_history_r)  # user-item network\n",
    "\n",
    "        self_feats = self.features.weight[nodes]\n",
    "        # self-connection could be considered.\n",
    "        combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
    "        combined = F.relu(self.linear1(combined))\n",
    "\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2c3ab831",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_u_history = UV_Encoder(u2e, embed_dim, history_u_lists, history_ur_lists, agg_u_history, cuda=device, uv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e4f64e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UV_Encoder(\n",
       "  (features): Embedding(705, 128)\n",
       "  (aggregator): UV_Aggregator(\n",
       "    (v2e): Embedding(1941, 128)\n",
       "    (r2e): Embedding(8, 128)\n",
       "    (u2e): Embedding(705, 128)\n",
       "    (w_r1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (w_r2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (att): Attention(\n",
       "      (bilinear): Bilinear(in1_features=128, in2_features=128, out_features=1, bias=True)\n",
       "      (att1): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (att2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (att3): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (softmax): Softmax(dim=0)\n",
       "    )\n",
       "  )\n",
       "  (linear1): Linear(in_features=256, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_u_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c996b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "#from Attention import Attention\n",
    "\n",
    "\n",
    "class Social_Aggregator(nn.Module):\n",
    "    \"\"\"\n",
    "    Social Aggregator: for aggregating embeddings of social neighbors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, u2e, embed_dim, cuda=\"cpu\"):\n",
    "        super(Social_Aggregator, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.device = cuda\n",
    "        self.u2e = u2e\n",
    "        self.embed_dim = embed_dim\n",
    "        self.att = Attention(self.embed_dim)\n",
    "\n",
    "    def forward(self, nodes, to_neighs):\n",
    "        embed_matrix = torch.empty(len(nodes), self.embed_dim, dtype=torch.float).to(self.device)\n",
    "        for i in range(len(nodes)):\n",
    "            tmp_adj = to_neighs[i]\n",
    "            num_neighs = len(tmp_adj)\n",
    "            # \n",
    "            e_u = self.u2e.weight[list(tmp_adj)] # fast: user embedding \n",
    "            #slow: item-space user latent factor (item aggregation)\n",
    "            #feature_neigbhors = self.features(torch.LongTensor(list(tmp_adj)).to(self.device))\n",
    "            #e_u = torch.t(feature_neigbhors)\n",
    "\n",
    "            u_rep = self.u2e.weight[nodes[i]]\n",
    "\n",
    "            att_w = self.att(e_u, u_rep, num_neighs)\n",
    "            att_history = torch.mm(e_u.t(), att_w).t()\n",
    "            embed_matrix[i] = att_history\n",
    "        to_feats = embed_matrix\n",
    "\n",
    "        return to_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7ad6b3cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(nodes)>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda nodes: enc_u_history(nodes).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0975236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_u_social = Social_Aggregator(lambda nodes: enc_u_history(nodes).t(), u2e, embed_dim, cuda=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2b3ea45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Social_Aggregator(\n",
       "  (u2e): Embedding(705, 128)\n",
       "  (att): Attention(\n",
       "    (bilinear): Bilinear(in1_features=128, in2_features=128, out_features=1, bias=True)\n",
       "    (att1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (att2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (att3): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (softmax): Softmax(dim=0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_u_social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8e9991d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Social_Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features, embed_dim, social_adj_lists, aggregator, base_model=None, cuda=\"cpu\"):\n",
    "        super(Social_Encoder, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.social_adj_lists = social_adj_lists\n",
    "        self.aggregator = aggregator\n",
    "        if base_model != None:\n",
    "            self.base_model = base_model\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = cuda\n",
    "        self.linear1 = nn.Linear(2 * self.embed_dim, self.embed_dim)  #\n",
    "\n",
    "    def forward(self, nodes):\n",
    "\n",
    "        to_neighs = []\n",
    "        for node in nodes:\n",
    "            to_neighs.append(self.social_adj_lists[int(node)])\n",
    "        neigh_feats = self.aggregator.forward(nodes, to_neighs)  # user-user network\n",
    "        #print(type(self_feats))\n",
    "        self_feats = self.features(torch.LongTensor(nodes.cpu().numpy())).to(self.device)\n",
    "        #print(type(self_feats))\n",
    "        self_feats = self_feats.t()\n",
    "        \n",
    "        # self-connection could be considered.\n",
    "        combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
    "        combined = F.relu(self.linear1(combined))\n",
    "\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "85133a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_u = Social_Encoder(lambda nodes: enc_u_history(nodes).t(), embed_dim, social_adj_lists, agg_u_social, base_model=enc_u_history, cuda=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "69ffb10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Social_Encoder(\n",
       "  (aggregator): Social_Aggregator(\n",
       "    (u2e): Embedding(705, 128)\n",
       "    (att): Attention(\n",
       "      (bilinear): Bilinear(in1_features=128, in2_features=128, out_features=1, bias=True)\n",
       "      (att1): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (att2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (att3): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (softmax): Softmax(dim=0)\n",
       "    )\n",
       "  )\n",
       "  (base_model): UV_Encoder(\n",
       "    (features): Embedding(705, 128)\n",
       "    (aggregator): UV_Aggregator(\n",
       "      (v2e): Embedding(1941, 128)\n",
       "      (r2e): Embedding(8, 128)\n",
       "      (u2e): Embedding(705, 128)\n",
       "      (w_r1): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (w_r2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (att): Attention(\n",
       "        (bilinear): Bilinear(in1_features=128, in2_features=128, out_features=1, bias=True)\n",
       "        (att1): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (att2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (att3): Linear(in_features=128, out_features=1, bias=True)\n",
       "        (softmax): Softmax(dim=0)\n",
       "      )\n",
       "    )\n",
       "    (linear1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=256, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0a40329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # item feature: user * rating\n",
    "agg_v_history = UV_Aggregator(v2e, r2e, u2e, embed_dim, cuda=device, uv=False)\n",
    "enc_v_history = UV_Encoder(v2e, embed_dim, history_v_lists, history_vr_lists, agg_v_history, cuda=device, uv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "523c8e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UV_Aggregator(\n",
       "  (v2e): Embedding(1941, 128)\n",
       "  (r2e): Embedding(8, 128)\n",
       "  (u2e): Embedding(705, 128)\n",
       "  (w_r1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (w_r2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (att): Attention(\n",
       "    (bilinear): Bilinear(in1_features=128, in2_features=128, out_features=1, bias=True)\n",
       "    (att1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (att2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (att3): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (softmax): Softmax(dim=0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_v_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "616bff16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UV_Encoder(\n",
       "  (features): Embedding(1941, 128)\n",
       "  (aggregator): UV_Aggregator(\n",
       "    (v2e): Embedding(1941, 128)\n",
       "    (r2e): Embedding(8, 128)\n",
       "    (u2e): Embedding(705, 128)\n",
       "    (w_r1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (w_r2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (att): Attention(\n",
       "      (bilinear): Bilinear(in1_features=128, in2_features=128, out_features=1, bias=True)\n",
       "      (att1): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (att2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (att3): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (softmax): Softmax(dim=0)\n",
       "    )\n",
       "  )\n",
       "  (linear1): Linear(in_features=256, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_v_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6236e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRec(nn.Module):\n",
    "\n",
    "    def __init__(self, enc_u, enc_v_history, r2e):\n",
    "        super(GraphRec, self).__init__()\n",
    "        self.enc_u = enc_u\n",
    "        self.enc_v_history = enc_v_history\n",
    "        self.embed_dim = enc_u.embed_dim\n",
    "\n",
    "        self.w_ur1 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.w_ur2 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.w_vr1 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.w_vr2 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.w_uv1 = nn.Linear(self.embed_dim * 2, self.embed_dim)\n",
    "        self.w_uv2 = nn.Linear(self.embed_dim, 16)\n",
    "        self.w_uv3 = nn.Linear(16, 1)\n",
    "        self.r2e = r2e\n",
    "        self.bn1 = nn.BatchNorm1d(self.embed_dim, momentum=0.5)\n",
    "        self.bn2 = nn.BatchNorm1d(self.embed_dim, momentum=0.5)\n",
    "        self.bn3 = nn.BatchNorm1d(self.embed_dim, momentum=0.5)\n",
    "        self.bn4 = nn.BatchNorm1d(16, momentum=0.5)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, nodes_u, nodes_v):\n",
    "        embeds_u = self.enc_u(nodes_u)\n",
    "        embeds_v = self.enc_v_history(nodes_v)\n",
    "\n",
    "        x_u = F.relu(self.bn1(self.w_ur1(embeds_u)))\n",
    "        x_u = F.dropout(x_u, training=self.training)\n",
    "        x_u = self.w_ur2(x_u)\n",
    "        x_v = F.relu(self.bn2(self.w_vr1(embeds_v)))\n",
    "        x_v = F.dropout(x_v, training=self.training)\n",
    "        x_v = self.w_vr2(x_v)\n",
    "\n",
    "        x_uv = torch.cat((x_u, x_v), 1)\n",
    "        x = F.relu(self.bn3(self.w_uv1(x_uv)))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.bn4(self.w_uv2(x)))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        scores = self.w_uv3(x)\n",
    "        return scores.squeeze()\n",
    "\n",
    "    def loss(self, nodes_u, nodes_v, labels_list):\n",
    "        scores = self.forward(nodes_u, nodes_v)\n",
    "        return self.criterion(scores, labels_list)\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, best_rmse, best_mae):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        batch_nodes_u, batch_nodes_v, labels_list = data\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(batch_nodes_u.to(device), batch_nodes_v.to(device), labels_list.to(device))\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print('[%d, %5d] loss: %.3f, The best rmse/mae: %.6f / %.6f' % (\n",
    "                epoch, i, running_loss / 100, best_rmse, best_mae))\n",
    "            running_loss = 0.0\n",
    "    return 0\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    tmp_pred = []\n",
    "    target = []\n",
    "    with torch.no_grad():\n",
    "        for test_u, test_v, tmp_target in test_loader:\n",
    "            test_u, test_v, tmp_target = test_u.to(device), test_v.to(device), tmp_target.to(device)\n",
    "            val_output = model.forward(test_u, test_v)\n",
    "            tmp_pred.append(list(val_output.data.cpu().numpy()))\n",
    "            target.append(list(tmp_target.data.cpu().numpy()))\n",
    "    tmp_pred = np.array(sum(tmp_pred, []))\n",
    "    target = np.array(sum(target, []))\n",
    "    expected_rmse = sqrt(mean_squared_error(tmp_pred, target))\n",
    "    mae = mean_absolute_error(tmp_pred, target)\n",
    "    return expected_rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d6476956",
   "metadata": {},
   "outputs": [],
   "source": [
    " # model\n",
    "    \n",
    "lr = 0.01\n",
    "graphrec = GraphRec(enc_u, enc_v_history, r2e).to(device)\n",
    "optimizer = torch.optim.RMSprop(graphrec.parameters(), lr=lr, alpha=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a4788fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rmse = 9999.0\n",
    "best_mae = 9999.0\n",
    "endure_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "969bfcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     0] loss: 0.096, The best rmse/mae: 9999.000000 / 9999.000000\n",
      "[1,   100] loss: 2.047, The best rmse/mae: 9999.000000 / 9999.000000\n",
      "rmse: 1.5227, mae:1.3479 \n",
      "[2,     0] loss: 0.013, The best rmse/mae: 1.522698 / 1.347895\n",
      "[2,   100] loss: 1.032, The best rmse/mae: 1.522698 / 1.347895\n",
      "rmse: 0.9026, mae:0.7445 \n",
      "[3,     0] loss: 0.010, The best rmse/mae: 0.902622 / 0.744537\n",
      "[3,   100] loss: 0.819, The best rmse/mae: 0.902622 / 0.744537\n",
      "rmse: 1.2216, mae:1.0759 \n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    train(graphrec, device, train_loader, optimizer, epoch, best_rmse, best_mae)\n",
    "    expected_rmse, mae = test(graphrec, device, test_loader)\n",
    "    # please add the validation set to tune the hyper-parameters based on your datasets.\n",
    "\n",
    "    # early stopping (no validation set in toy dataset)\n",
    "    if best_rmse > expected_rmse:\n",
    "        best_rmse = expected_rmse\n",
    "        best_mae = mae\n",
    "        endure_count = 0\n",
    "    else:\n",
    "        endure_count += 1\n",
    "    print(\"rmse: %.4f, mae:%.4f \" % (expected_rmse, mae))\n",
    "\n",
    "    if endure_count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0ab4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b530a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aa1290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2c4a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fc1527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bdea5ef",
   "metadata": {},
   "source": [
    "### 처음 시작하는 부분 노드 128개 시작 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6771e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphrec.train()\n",
    "running_loss = 0.0\n",
    "for i, data in enumerate(train_loader, 0):\n",
    "    batch_nodes_u, batch_nodes_v, labels_list = data\n",
    "    optimizer.zero_grad()\n",
    "    break\n",
    "#     loss = model.loss(batch_nodes_u.to(device), batch_nodes_v.to(device), labels_list.to(device))\n",
    "#     loss.backward(retain_graph=True)\n",
    "#     optimizer.step()\n",
    "#     running_loss += loss.item()\n",
    "#     if i % 100 == 0:\n",
    "#         print('[%d, %5d] loss: %.3f, The best rmse/mae: %.6f / %.6f' % (\n",
    "#             epoch, i, running_loss / 100, best_rmse, best_mae))\n",
    "#         running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "19d3e510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([280, 211, 615, 343, 479, 673, 124, 280,  99, 366, 504,  78, 267, 412,\n",
       "         647, 430, 703, 143, 151,  96, 365, 399,  10, 590,  24, 281, 335, 525,\n",
       "         294, 429, 603, 379, 300, 177, 247, 528, 440, 341, 193, 286, 245,  51,\n",
       "         669, 386, 204, 103, 265, 300, 253, 464, 524, 356, 646, 458,  58, 200,\n",
       "         298,  78, 669, 424, 638, 626, 126, 341, 694, 699, 169,  44,  36, 247,\n",
       "          53, 486, 293, 517, 693, 435,  54,  75, 277,  81, 172, 197, 256, 420,\n",
       "         242, 513, 549,  51,  38, 193, 171, 172, 611, 479,  43, 479, 628, 226,\n",
       "         625, 227, 430, 568, 172, 315, 422,  49, 468, 683, 669,  74, 680, 468,\n",
       "          13, 156, 637, 484,  28, 243, 113, 360,  68, 292,  68,  75,  74, 259,\n",
       "         190,  37]),\n",
       " tensor([  32,   47,   47,   37,   64,   76,   58,   24,   48,   88,   61,  607,\n",
       "          114,   52,  718,   34,   51,  211, 1847,   74,   51,   11,   24,   80,\n",
       "          255,    8,   78,    8,  122,   13,    6,  102,   46, 1081,   74,   47,\n",
       "           45, 1255,  727,  122,   47, 1455,  444,  690,   10,  173,   52,  132,\n",
       "           47,   20, 1413,   13,   23,   31,   32,   24,   26,  177,  122,  144,\n",
       "           34,  282,    6, 1411,   65,   11,   74,   68,   26,   62, 1547,   31,\n",
       "           24,   41,   45,   86,   11,   27,   88,  119,   27,   80,  889,    0,\n",
       "          122,  102,   86,  120,  810, 1450,   31,  300,   78,   31,  170,   22,\n",
       "          754,    0,  287,   58,   46,   60,  255,  524,   39,   24,   55,   62,\n",
       "          330,  295,   67,   39,   24,   68,   90,   42,   69,   52,   74,   58,\n",
       "          951,   63,  515, 1000,  143,  239,  300,  358]),\n",
       " tensor([4.0000, 1.0000, 3.5000, 2.5000, 2.5000, 3.0000, 4.0000, 3.0000, 4.0000,\n",
       "         3.5000, 2.0000, 3.5000, 2.0000, 4.0000, 0.5000, 3.5000, 2.5000, 3.0000,\n",
       "         3.0000, 3.0000, 2.5000, 4.0000, 4.0000, 0.5000, 4.0000, 4.0000, 3.5000,\n",
       "         4.0000, 4.0000, 3.5000, 3.5000, 4.0000, 2.5000, 1.5000, 2.5000, 2.5000,\n",
       "         4.0000, 3.5000, 2.5000, 2.5000, 2.0000, 4.0000, 3.5000, 4.0000, 2.5000,\n",
       "         4.0000, 3.5000, 3.0000, 1.0000, 2.5000, 3.5000, 4.0000, 3.5000, 3.5000,\n",
       "         3.0000, 1.0000, 2.0000, 3.0000, 3.5000, 3.5000, 4.0000, 4.0000, 3.5000,\n",
       "         3.0000, 4.0000, 3.0000, 4.0000, 3.5000, 3.5000, 3.5000, 3.0000, 3.5000,\n",
       "         3.0000, 4.0000, 4.0000, 2.5000, 3.5000, 3.0000, 2.0000, 3.5000, 3.5000,\n",
       "         4.0000, 4.0000, 4.0000, 4.0000, 4.0000, 2.0000, 1.0000, 3.0000, 1.5000,\n",
       "         3.0000, 4.0000, 4.0000, 4.0000, 4.0000, 2.5000, 0.5000, 1.5000, 3.0000,\n",
       "         3.0000, 3.5000, 1.0000, 4.0000, 2.0000, 4.0000, 3.5000, 3.0000, 3.5000,\n",
       "         4.0000, 2.0000, 3.0000, 3.0000, 3.0000, 3.0000, 3.0000, 3.0000, 3.0000,\n",
       "         2.0000, 2.0000, 2.0000, 4.0000, 2.5000, 4.0000, 4.0000, 0.5000, 4.0000,\n",
       "         3.5000, 2.0000]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_nodes_u, batch_nodes_v, labels_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d998da30",
   "metadata": {},
   "source": [
    "### UV_Encoder 들어가는 부분 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "57edac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1aef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "graphrec.train()\n",
    "running_loss = 0.0\n",
    "\n",
    "# 128 batch 만큼 (노드 128개 들어감)\n",
    "for i, data in enumerate(train_loader, 0):\n",
    "    batch_nodes_u, batch_nodes_v, labels_list = data\n",
    "    optimizer.zero_grad()\n",
    "    break \n",
    "    \n",
    "\n",
    "\n",
    "tmp_history_uv = []\n",
    "tmp_history_r = []\n",
    "for node in batch_nodes_u:\n",
    "    tmp_history_uv.append(history_u_lists[int(node)])\n",
    "    tmp_history_r.append(history_ur_lists[int(node)])\n",
    "# neigh_feats = self.aggregator.forward(nodes, tmp_history_uv, tmp_history_r)  # user-item network \n",
    "# 위에 부분을 사용한 것. \n",
    "neigh_feats = agg_u_history.forward(batch_nodes_u, tmp_history_uv, tmp_history_r)\n",
    "\n",
    "# class UV_Encoder(nn.Module):\n",
    "#   def forward(self, nodes):\n",
    "# forward 부분 \n",
    "\n",
    "embed_dim = 128\n",
    "linear1 = nn.Linear(2 * embed_dim, embed_dim) # concat 하니깐 2*embed_dim 하는 거 \n",
    "\n",
    "self_feats = u2e.weight[batch_nodes_u]\n",
    "# self-connection could be considered.\n",
    "\n",
    "# (torch.Tensor, torch.Size([128, 256]))\n",
    "combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
    "\n",
    "combined = combined.to('cpu')\n",
    "\n",
    "#  torch.Size([128, 128]))\n",
    "combined = F.relu(linear1(combined))\n",
    "\n",
    "####################################################################################################\n",
    "class UV_Encoder(nn.Module): 이 부분에서  \n",
    "\n",
    "aggregation하고서 나온거를 concat해서 128 * 256 사이즈를 다시 렐루와 linear들어가서 128*128로 처리 \n",
    "\n",
    "enc_u_history == combined 가 됨. \n",
    "####################################################################################################\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8c5fbabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_history_uv = []\n",
    "tmp_history_r = []\n",
    "for node in batch_nodes_u:\n",
    "    tmp_history_uv.append(history_u_lists[int(node)])\n",
    "    tmp_history_r.append(history_ur_lists[int(node)])\n",
    "\n",
    "len(tmp_history_uv), len(tmp_history_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8dfd7112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp_history_uv), len(tmp_history_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6ed85a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_len_history_uv = []\n",
    "tmp_len_history_r = []\n",
    "for i in range(128):\n",
    "    tmp_len_history_uv.append(len(tmp_history_uv[i]))\n",
    "    tmp_len_history_r.append(len(tmp_history_r[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "99d5ff2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 30, 5, 38, 42, 35, 17, 33, 12, 37, 39, 94, 22, 10, 16, 19, 37, 31, 79, 16, 39, 13, 21, 36, 37, 13, 12, 29, 10, 15, 15, 16, 37, 33, 11, 43, 13, 55, 133, 15, 14, 34, 110, 14, 25, 4, 20, 37, 15, 40, 89, 28, 34, 29, 48, 39, 37, 94, 110, 38, 11, 37, 42, 55, 35, 34, 21, 22, 41, 11, 211, 25, 7, 8, 23, 33, 29, 76, 31, 81, 63, 26, 52, 35, 34, 4, 39, 34, 161, 133, 34, 63, 18, 42, 102, 42, 102, 10, 49, 33, 19, 34, 63, 47, 11, 36, 14, 16, 110, 146, 38, 14, 62, 9, 81, 29, 20, 3, 26, 34, 6, 64, 6, 76, 146, 48, 70, 64]"
     ]
    }
   ],
   "source": [
    "# 각각의 노드가 연결된 수 \n",
    "print(tmp_len_history_uv, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ad2f0ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 30, 5, 38, 42, 35, 17, 33, 12, 37, 39, 94, 22, 10, 16, 19, 37, 31, 79, 16, 39, 13, 21, 36, 37, 13, 12, 29, 10, 15, 15, 16, 37, 33, 11, 43, 13, 55, 133, 15, 14, 34, 110, 14, 25, 4, 20, 37, 15, 40, 89, 28, 34, 29, 48, 39, 37, 94, 110, 38, 11, 37, 42, 55, 35, 34, 21, 22, 41, 11, 211, 25, 7, 8, 23, 33, 29, 76, 31, 81, 63, 26, 52, 35, 34, 4, 39, 34, 161, 133, 34, 63, 18, 42, 102, 42, 102, 10, 49, 33, 19, 34, 63, 47, 11, 36, 14, 16, 110, 146, 38, 14, 62, 9, 81, 29, 20, 3, 26, 34, 6, 64, 6, 76, 146, 48, 70, 64]\n",
      "\n",
      "[[7, 3, 3, 3, 0, 5, 2, 4, 2, 0, 2, 3, 4, 3, 5, 3, 3, 3, 5, 2, 2, 5, 5, 2, 6, 5, 3, 2, 4, 2, 6, 5, 7]]"
     ]
    }
   ],
   "source": [
    "# batch수 만큼의 노드에 연결된 rating의 수 (tmp_len_history_uv와 동일한 숫자로 rating이 되어진 것을 확인가능) 즉 관계들의 숫자가 동일하게 구성됨 \n",
    "print(tmp_len_history_r, end=\"\")\n",
    "print('\\n')\n",
    "\n",
    "# 한 노드가 연결된 아이템들의 rating 값들 \n",
    "print(tmp_history_r[:1], end=\"\") # 33개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f7591d",
   "metadata": {},
   "source": [
    "### UV_Encoder의 aggreagation(agg_u_history)의 forward 들어가는 부분 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "36d9c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neigh_feats = self.aggregator.forward(nodes, tmp_history_uv, tmp_history_r)  # user-item network \n",
    "# 위에 부분을 사용한 것. \n",
    "neigh_feats = agg_u_history.forward(batch_nodes_u, tmp_history_uv, tmp_history_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "247746ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 128]),\n",
       " tensor([[0.0368, 0.2152, 0.0079, 0.0601, 0.0270, 0.1075, 0.0042, 0.1858, 0.0345,\n",
       "          0.0157, 0.0770, 0.0823, 0.2317, 0.0419, 0.0494, 0.1833, 0.1359, 0.1265,\n",
       "          0.1890, 0.0112, 0.0667, 0.3008, 0.2181, 0.0790, 0.1828, 0.0664, 0.0842,\n",
       "          0.1432, 0.0167, 0.0796, 0.0694, 0.0050, 0.0246, 0.0328, 0.0922, 0.1618,\n",
       "          0.0323, 0.0953, 0.0251, 0.0646, 0.0178, 0.0130, 0.1527, 0.0489, 0.0074,\n",
       "          0.0468, 0.3014, 0.0379, 0.2098, 0.1054, 0.3498, 0.0567, 0.0335, 0.0480,\n",
       "          0.0094, 0.1064, 0.0458, 0.1316, 0.2392, 0.1750, 0.0369, 0.1767, 0.0317,\n",
       "          0.0201, 0.0899, 0.2086, 0.0329, 0.1658, 0.1831, 0.3152, 0.0166, 0.1058,\n",
       "          0.0984, 0.0012, 0.0588, 0.0386, 0.1685, 0.0260, 0.1205, 0.0406, 0.0243,\n",
       "          0.0881, 0.0000, 0.1067, 0.1058, 0.2552, 0.1534, 0.1918, 0.1632, 0.0924,\n",
       "          0.0524, 0.0812, 0.1306, 0.2622, 0.2975, 0.0103, 0.0181, 0.2124, 0.0035,\n",
       "          0.0923, 0.0144, 0.2624, 0.0423, 0.0780, 0.0732, 0.0455, 0.2509, 0.0264,\n",
       "          0.0454, 0.0615, 0.1437, 0.1532, 0.1365, 0.1269, 0.1736, 0.1660, 0.0538,\n",
       "          0.0351, 0.0812, 0.0421, 0.0159, 0.0693, 0.1234, 0.0560, 0.0154, 0.0969,\n",
       "          0.0000, 0.3769]], device='cuda:0', grad_fn=<SliceBackward>))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh_feats.shape, neigh_feats[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "649f0a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# class UV_Encoder(nn.Module):\n",
    "#   def forward(self, nodes):\n",
    "# forward 부분 \n",
    "\n",
    "class UV_Encoder(nn.Module): 이 부분에서  \n",
    "\n",
    "aggregation하고서 나온거를 concat해서 128 * 256 사이즈를 다시 렐루와 linear들어가서 128*128로 처리 \n",
    "\n",
    "enc_u_history == combined 가 됨. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "embed_dim = 128\n",
    "linear1 = nn.Linear(2 * embed_dim, embed_dim)\n",
    "\n",
    "self_feats = u2e.weight[batch_nodes_u]\n",
    "# self-connection could be considered.\n",
    "\n",
    "# (torch.Tensor, torch.Size([128, 256]))\n",
    "combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
    "\n",
    "combined = combined.to('cpu')\n",
    "combined = F.relu(linear1(combined))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "342a91c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4204, 0.0000, 0.0154,  ..., 0.5163, 0.0000, 0.3760],\n",
       "        [0.2692, 0.3195, 0.0000,  ..., 0.0123, 0.4432, 0.0000],\n",
       "        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0427],\n",
       "        ...,\n",
       "        [0.0000, 0.1793, 0.0976,  ..., 0.0000, 0.0000, 0.1134],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.4487, 0.0000, 0.0138],\n",
       "        [0.1241, 0.7374, 0.0360,  ..., 0.0818, 0.5448, 0.0000]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c786f",
   "metadata": {},
   "source": [
    "### Social_Encoder 클래스 시작 (UV_Encoder 클래스 이후에 combined 값이 enc_u_history 부분임)\n",
    "###### enc_u_history이게 Social_Encode의 base_model로 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1faa27b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "features = lambda nodes: enc_u_history(nodes).t() ## features # lambda nodes: enc_u_history(nodes).t()\n",
    "social_adj_lists = social_adj_lists ## social_adj_lists # social_adj_lists\n",
    "aggregator = agg_u_social ## aggregator # agg_u_social\n",
    "base_model = enc_u_history ##base_model # enc_u_history\n",
    "embed_dim = embed_dim # 128\n",
    "device  = device\n",
    "linear1 = nn.Linear(2 * embed_dim, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "to_neighs = []\n",
    "for node in batch_nodes_u:\n",
    "    to_neighs.append(social_adj_lists[int(node)])\n",
    "neigh_feats = aggregator.forward(batch_nodes_u, to_neighs)  # user-user network\n",
    "#print(type(self_feats))\n",
    "self_feats = features(torch.LongTensor(batch_nodes_u.cpu().numpy())).to(device)\n",
    "#print(type(self_feats))\n",
    "self_feats = self_feats.t()\n",
    "\n",
    "# self-connection could be considered.\n",
    "combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
    "combined = F.relu(self.linear1(combined))\n",
    "\n",
    "combined\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0edc4217",
   "metadata": {},
   "outputs": [],
   "source": [
    "## social_adj_lists: user's connected neighborhoods\n",
    "\n",
    "to_neighs = []\n",
    "for node in batch_nodes_u:\n",
    "    to_neighs.append(social_adj_lists[int(node)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bb9b4299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{13, 23, 28, 29, 41, 42, 43, 44, 281, 340, 341},\n",
       " {79, 98, 209, 210, 331, 332, 682},\n",
       " {411},\n",
       " {336},\n",
       " {295, 418}]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_neighs[:5] # 한 유저가 다른 유저에게 연결된 social 정보를 가지고 있습니다. 여기선 예시로 5개 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a53dfbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_feats = aggregator.forward(batch_nodes_u, to_neighs)  # user-user network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d4373a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 128]),\n",
       " tensor([-0.3405, -0.0851, -0.2990, -0.3371, -0.3064, -0.1205,  0.0521,  0.3534,\n",
       "         -0.2463, -0.3917,  0.1232, -0.3164, -0.0584, -0.3660,  0.0386,  0.1416,\n",
       "          0.1063,  0.0278,  0.3919,  0.1011, -0.5480,  0.3635, -0.0839, -0.0624,\n",
       "          0.2265,  0.0551, -0.0936, -0.0104,  0.1786,  0.0962, -0.3777,  0.1163,\n",
       "         -0.2015,  0.1075, -0.0527, -0.2682, -0.3329,  0.4912,  0.0710,  0.3826,\n",
       "         -0.5388,  0.0317, -0.2323,  0.1493, -0.1689, -0.2612, -0.2491,  0.0289,\n",
       "          0.3895, -0.3353,  0.3523,  0.0597, -0.1507, -0.0290,  0.3689, -0.3850,\n",
       "          0.0025,  0.1479, -0.1980,  0.2675,  0.0015,  0.0585,  0.3016,  0.0732,\n",
       "          0.2831, -0.1190,  0.0632, -0.0672, -0.3708,  0.3467, -0.3639,  0.5488,\n",
       "         -0.5733,  0.2925,  0.1091, -0.0438,  0.4186, -0.2221,  0.0546, -0.0355,\n",
       "         -0.0031, -0.4996, -0.3259, -0.4009, -0.1001, -0.2260,  0.3206, -0.0706,\n",
       "         -0.0911,  0.0814, -0.0136,  0.3477,  0.2614, -0.3526,  0.0723, -0.0311,\n",
       "         -0.3485,  0.3041, -0.0342,  0.5675, -0.0923,  0.0088, -0.1966,  0.0911,\n",
       "         -0.3428,  0.0791, -0.4094, -0.5522, -0.2544, -0.0525,  0.1399, -0.5002,\n",
       "          0.5370,  0.1815,  0.0754,  0.3021, -0.3414, -0.2720, -0.3637,  0.1399,\n",
       "         -0.5902,  0.1806, -0.0438,  0.2014,  0.3966,  0.0875,  0.0759,  0.2256],\n",
       "        device='cuda:0', grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 소셜정보 aggregation 한 결과 \n",
    "neigh_feats.shape, neigh_feats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9eee67b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1304, 0.0000, 0.0000, 1.1121, 0.4706, 0.4266, 0.3092, 0.1302, 0.7443,\n",
       "         0.0000, 0.0054, 0.0000, 0.3650, 0.0000, 0.4820, 0.0000, 0.0000, 0.0000,\n",
       "         0.0271, 0.0000, 0.8191, 0.0000, 0.2040, 0.2169, 0.0000, 0.3003, 0.0000,\n",
       "         0.0656, 0.0000, 0.0356, 0.4538, 0.5412, 0.3139, 0.3672, 0.0000, 0.0000,\n",
       "         0.0000, 0.5696, 0.0000, 0.2573, 0.0000, 0.5056, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.3141, 0.0000, 0.0752, 0.0000, 0.0000, 0.3096, 0.4184,\n",
       "         0.3706, 0.5858, 0.0031, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5603,\n",
       "         0.5714, 0.0000, 0.0000, 0.3961, 0.0000, 0.0229, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.2193, 0.0000, 0.1474, 0.0000, 0.2324, 0.0468, 0.0000, 0.5453,\n",
       "         0.0000, 0.0476, 0.5499, 0.4049, 0.0998, 0.2995, 0.5056, 0.1710, 0.0000,\n",
       "         0.0000, 0.5456, 0.3242, 0.4701, 0.5687, 0.4691, 0.0000, 0.5389, 0.1410,\n",
       "         0.5128, 0.0000, 0.0000, 0.5462, 0.0000, 0.5890, 0.0000, 0.0000, 0.4968,\n",
       "         0.0000, 0.2873, 0.0000, 0.0000, 0.0740, 0.0000, 0.4401, 0.1808, 0.0000,\n",
       "         0.0000, 0.3236, 0.0000, 0.1448, 0.5722, 0.1431, 0.2315, 0.2876, 0.3317,\n",
       "         0.0000, 0.0000], device='cuda:0', grad_fn=<SelectBackward>),\n",
       " torch.Size([128, 128]))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(type(self_feats))\n",
    "# features ====> lambda nodes: enc_u_history(nodes).t()\n",
    "# batch_nodes_u의 전치행렬이 들어감  (enc_u_history(batch_nodes_u).t()[0]와 같음)\n",
    "self_feats = features(torch.LongTensor(batch_nodes_u.cpu().numpy())).to(device)\n",
    "self_feats[0], self_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d5deb314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(self_feats))\n",
    "self_feats = self_feats.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6ae18ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-connection could be considered.\n",
    "# combined ==> torch.Size([128, 256])\n",
    "combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
    "# combined ==> torch.Size([128, 128])\n",
    "combined = combined.to('cpu')\n",
    "combined = F.relu(linear1(combined))\n",
    "\n",
    "enc_u = combined #　enc_u == combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "92bca59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0700, 0.0000, 0.1852,  ..., 0.1006, 0.0000, 0.1970],\n",
       "        [0.0000, 0.0243, 0.0157,  ..., 0.0000, 0.3118, 0.3124],\n",
       "        [0.0000, 0.1815, 0.0000,  ..., 0.1086, 0.2316, 0.0000],\n",
       "        ...,\n",
       "        [0.1322, 0.0000, 0.0000,  ..., 0.0090, 0.0000, 0.0000],\n",
       "        [0.0000, 0.2337, 0.2874,  ..., 0.3724, 0.0000, 0.0000],\n",
       "        [0.0078, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1757]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f3e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRec(nn.Module):\n",
    "\n",
    "    def __init__(self, enc_u, enc_v_history, r2e):\n",
    "        super(GraphRec, self).__init__()\n",
    "        self.enc_u = enc_u\n",
    "        self.enc_v_history = enc_v_history\n",
    "        self.embed_dim = enc_u.embed_dim\n",
    "\n",
    "        self.w_ur1 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.w_ur2 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.w_vr1 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.w_vr2 = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.w_uv1 = nn.Linear(self.embed_dim * 2, self.embed_dim)\n",
    "        self.w_uv2 = nn.Linear(self.embed_dim, 16)\n",
    "        self.w_uv3 = nn.Linear(16, 1)\n",
    "        self.r2e = r2e\n",
    "        self.bn1 = nn.BatchNorm1d(self.embed_dim, momentum=0.5)\n",
    "        self.bn2 = nn.BatchNorm1d(self.embed_dim, momentum=0.5)\n",
    "        self.bn3 = nn.BatchNorm1d(self.embed_dim, momentum=0.5)\n",
    "        self.bn4 = nn.BatchNorm1d(16, momentum=0.5)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, nodes_u, nodes_v):\n",
    "        embeds_u = self.enc_u(nodes_u)\n",
    "        embeds_v = self.enc_v_history(nodes_v)\n",
    "\n",
    "        x_u = F.relu(self.bn1(self.w_ur1(embeds_u)))\n",
    "        x_u = F.dropout(x_u, training=self.training)\n",
    "        x_u = self.w_ur2(x_u)\n",
    "        x_v = F.relu(self.bn2(self.w_vr1(embeds_v)))\n",
    "        x_v = F.dropout(x_v, training=self.training)\n",
    "        x_v = self.w_vr2(x_v)\n",
    "\n",
    "        x_uv = torch.cat((x_u, x_v), 1)\n",
    "        x = F.relu(self.bn3(self.w_uv1(x_uv)))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.bn4(self.w_uv2(x)))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        scores = self.w_uv3(x)\n",
    "        return scores.squeeze()\n",
    "\n",
    "    def loss(self, nodes_u, nodes_v, labels_list):\n",
    "        scores = self.forward(nodes_u, nodes_v)\n",
    "        return self.criterion(scores, labels_list)\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, best_rmse, best_mae):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        batch_nodes_u, batch_nodes_v, labels_list = data\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(batch_nodes_u.to(device), batch_nodes_v.to(device), labels_list.to(device))\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print('[%d, %5d] loss: %.3f, The best rmse/mae: %.6f / %.6f' % (\n",
    "                epoch, i, running_loss / 100, best_rmse, best_mae))\n",
    "            running_loss = 0.0\n",
    "    return 0\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    tmp_pred = []\n",
    "    target = []\n",
    "    with torch.no_grad():\n",
    "        for test_u, test_v, tmp_target in test_loader:\n",
    "            test_u, test_v, tmp_target = test_u.to(device), test_v.to(device), tmp_target.to(device)\n",
    "            val_output = model.forward(test_u, test_v)\n",
    "            tmp_pred.append(list(val_output.data.cpu().numpy()))\n",
    "            target.append(list(tmp_target.data.cpu().numpy()))\n",
    "    tmp_pred = np.array(sum(tmp_pred, []))\n",
    "    target = np.array(sum(target, []))\n",
    "    expected_rmse = sqrt(mean_squared_error(tmp_pred, target))\n",
    "    mae = mean_absolute_error(tmp_pred, target)\n",
    "    return expected_rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f3b09227",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_u = enc_u\n",
    "enc_v_history = enc_v_history\n",
    "embed_dim = embed_dim\n",
    "\n",
    "w_ur1 = nn.Linear(embed_dim, embed_dim)\n",
    "w_ur2 = nn.Linear(embed_dim, embed_dim)\n",
    "w_vr1 = nn.Linear(embed_dim, embed_dim)\n",
    "w_vr2 = nn.Linear(embed_dim, embed_dim)\n",
    "w_uv1 = nn.Linear(embed_dim * 2, embed_dim)\n",
    "w_uv2 = nn.Linear(embed_dim, 16)\n",
    "w_uv3 = nn.Linear(16, 1)\n",
    "r2e = labels_list ## r2e # labels_list\n",
    "bn1 = nn.BatchNorm1d(embed_dim, momentum=0.5)\n",
    "bn2 = nn.BatchNorm1d(embed_dim, momentum=0.5)\n",
    "bn3 = nn.BatchNorm1d(embed_dim, momentum=0.5)\n",
    "bn4 = nn.BatchNorm1d(16, momentum=0.5)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "fd1a0da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeds_u = enc_u(nodes_u)\n",
    "#embeds_v = enc_v_history(nodes_v)\n",
    "\n",
    "x_u = F.relu(bn1(w_ur1(enc_u)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2760a845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "02b3cc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_u = w_ur2(x_u)\n",
    "x_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f69e190a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_v = F.relu(bn2(w_vr1(enc_v_history(batch_nodes_v).cpu())))\n",
    "x_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "9ef2ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_v = w_vr2(x_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c034a685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 256])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_uv = torch.cat((x_u, x_v), 1)\n",
    "x_uv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2dfe4447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = F.relu(bn3(w_uv1(x_uv)))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "62618080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 16])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = F.relu(bn4(w_uv2(x)))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "027a3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1]) squeeze()\n"
     ]
    }
   ],
   "source": [
    "scores = w_uv3(x)\n",
    "print(scores.shape)\n",
    "\n",
    "scores.squeeze()\n",
    "print(scores.shape, \"squeeze()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e3e276d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0702],\n",
       "        [-0.0099],\n",
       "        [ 0.1754],\n",
       "        [-0.0233],\n",
       "        [ 0.1246],\n",
       "        [-0.0512],\n",
       "        [-0.5902],\n",
       "        [ 0.3616],\n",
       "        [ 0.2391],\n",
       "        [ 0.6653],\n",
       "        [ 0.1989],\n",
       "        [ 0.5786],\n",
       "        [-0.0733],\n",
       "        [ 0.6730],\n",
       "        [-0.0751],\n",
       "        [ 0.0692],\n",
       "        [-0.2070],\n",
       "        [ 0.0930],\n",
       "        [ 0.3057],\n",
       "        [-0.1126],\n",
       "        [ 0.1526],\n",
       "        [ 0.1043],\n",
       "        [-0.1905],\n",
       "        [ 0.2896],\n",
       "        [ 0.0617],\n",
       "        [-0.0708],\n",
       "        [-0.0182],\n",
       "        [ 0.5053],\n",
       "        [-0.0786],\n",
       "        [-0.3031],\n",
       "        [ 0.6304],\n",
       "        [-0.1517],\n",
       "        [ 0.6497],\n",
       "        [ 1.0408],\n",
       "        [ 0.0346],\n",
       "        [ 0.0533],\n",
       "        [-0.2603],\n",
       "        [ 0.1876],\n",
       "        [-0.2263],\n",
       "        [-0.6179],\n",
       "        [-0.1443],\n",
       "        [-0.0993],\n",
       "        [ 0.4771],\n",
       "        [ 0.8111],\n",
       "        [ 0.2421],\n",
       "        [ 0.0863],\n",
       "        [ 0.8681],\n",
       "        [ 1.1338],\n",
       "        [ 0.5297],\n",
       "        [ 0.2577],\n",
       "        [ 0.0065],\n",
       "        [ 0.1068],\n",
       "        [-0.5251],\n",
       "        [ 0.2291],\n",
       "        [-0.1011],\n",
       "        [ 0.2407],\n",
       "        [ 0.3140],\n",
       "        [ 0.1823],\n",
       "        [ 0.2251],\n",
       "        [ 0.0657],\n",
       "        [-0.2203],\n",
       "        [-0.0673],\n",
       "        [ 0.4462],\n",
       "        [ 0.0167],\n",
       "        [ 0.2976],\n",
       "        [-0.4375],\n",
       "        [ 0.2671],\n",
       "        [ 0.4126],\n",
       "        [-0.0613],\n",
       "        [-0.6736],\n",
       "        [-1.1589],\n",
       "        [ 0.7567],\n",
       "        [ 0.0210],\n",
       "        [ 0.6609],\n",
       "        [-0.3174],\n",
       "        [-0.6042],\n",
       "        [-0.4916],\n",
       "        [ 0.0786],\n",
       "        [ 0.7956],\n",
       "        [-0.1632],\n",
       "        [ 0.3836],\n",
       "        [ 0.5536],\n",
       "        [ 0.3520],\n",
       "        [-0.4368],\n",
       "        [-0.2113],\n",
       "        [-0.3039],\n",
       "        [ 0.0895],\n",
       "        [ 0.6055],\n",
       "        [ 0.8326],\n",
       "        [-0.3621],\n",
       "        [ 0.1563],\n",
       "        [ 0.4877],\n",
       "        [ 0.1820],\n",
       "        [ 0.5114],\n",
       "        [-0.1262],\n",
       "        [ 0.6959],\n",
       "        [ 0.4734],\n",
       "        [-0.1391],\n",
       "        [ 0.3286],\n",
       "        [ 0.3264],\n",
       "        [ 0.7282],\n",
       "        [ 0.7761],\n",
       "        [ 0.6716],\n",
       "        [ 0.3645],\n",
       "        [ 0.2804],\n",
       "        [ 0.0439],\n",
       "        [ 0.3467],\n",
       "        [-0.0018],\n",
       "        [ 0.4641],\n",
       "        [ 0.6814],\n",
       "        [-0.6904],\n",
       "        [ 0.1140],\n",
       "        [ 0.2842],\n",
       "        [-0.4977],\n",
       "        [ 1.1861],\n",
       "        [ 0.3177],\n",
       "        [ 0.0341],\n",
       "        [ 0.3161],\n",
       "        [ 0.0182],\n",
       "        [-0.0414],\n",
       "        [-0.5652],\n",
       "        [ 0.2125],\n",
       "        [-0.1526],\n",
       "        [-0.0833],\n",
       "        [-0.1636],\n",
       "        [-0.0037],\n",
       "        [ 0.2830],\n",
       "        [ 0.3795]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "af1c42f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/graph-embedding/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.MSELoss()\n",
    "loss = loss_func(scores, labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "7787b15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.5524, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb46ea01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
